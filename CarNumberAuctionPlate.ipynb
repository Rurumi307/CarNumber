{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def generate_lucky_numbers():\n",
    "    \"\"\"\n",
    "    Generate lucky numbers based on specific criteria and target sums.\n",
    "    \n",
    "    This function generates lucky numbers by considering different combinations of four digits\n",
    "    and checking their sums against predefined target sums. It also assigns categories to\n",
    "    the lucky numbers based on certain conditions.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing generated lucky numbers and their categories.\n",
    "    \"\"\"\n",
    "    # Define the target sums and possible digits\n",
    "    target_sums = [8, 11, 13, 15, 16, 17, 18, 21, 23, 24, 25, 29, 31, 32, 33, 35, 37, 39]\n",
    "\n",
    "    # Define lucky number categories and their associated values\n",
    "    lucky_dict = {\n",
    "        '伏位': [11, 22, 33, 44, 66, 77, 88, 99],\n",
    "        '延年': [19, 91, 78, 87, 43, 34, 26, 62],\n",
    "        '生氣': [14, 41, 67, 76, 93, 39, 28, 82],\n",
    "        '天醫': [13, 31, 68, 86, 94, 49, 72, 27],\n",
    "    }\n",
    "\n",
    "    # Initialize a list to store lucky numbers\n",
    "    lucky_numbers = []\n",
    "\n",
    "    # Generate all possible combinations of four digits\n",
    "    combinations = itertools.product(range(10), repeat=4)\n",
    "\n",
    "    # Iterate through each combination\n",
    "    for combination in combinations:\n",
    "        if 4 not in combination:\n",
    "            total_sum = sum(combination)\n",
    "\n",
    "            # Check if the total sum matches a target sum\n",
    "            if total_sum in target_sums:\n",
    "                sorted_combination_int = ''.join(map(str, combination))\n",
    "\n",
    "                # Determine the lucky number category\n",
    "                category = \"普通\"\n",
    "                for key, values in lucky_dict.items():\n",
    "                    if int(sorted_combination_int) % 100 in values:\n",
    "                        category = key\n",
    "                        break\n",
    "                # Append the lucky number to the list\n",
    "                lucky_numbers.append({\"類別\": category, \"號碼\": sorted_combination_int, '總和': total_sum})\n",
    "    \n",
    "    # Create a DataFrame from the list of lucky numbers and drop duplicates\n",
    "    lucky_number_df = pd.DataFrame(lucky_numbers).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return lucky_number_df\n",
    "\n",
    "# Generate lucky numbers and save them to an Excel file\n",
    "lucky_numbers_df = generate_lucky_numbers()\n",
    "lucky_numbers_df.to_excel(\"./result/lucky_number.xlsx\", index=False)\n",
    "lucky_numbers_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7deb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "section_list = [{\"20\":\"臺北市區\"}, {\"40\":\"臺北區\"}, {\"50\":\"新竹區\"}, {\"60\":\"臺中區\"}, {\"70\":\"嘉義區\"}, {\"30\":\"高雄市區\"}, {\"80\":\"高雄區\"}]\n",
    "station_text = \"\"\"[\"20-臺北市區監理所\",\"21-士林監理站\",\"25-基隆監理站\",\"26-金門監理站\",\"28-連江監理站\"];\n",
    "[\"40-臺北區監理所\",\"41-板橋監理站\",\"43-宜蘭監理站\",\"44-花蓮監理站\",\"45-玉里監理分站\",\"46-蘆洲監理站\"];\n",
    "[\"50-新竹區監理所\",\"51-新竹市監理站\",\"52-桃園監理站\",\"53-中壢監理站\",\"54-苗栗監理站\"];\n",
    "[\"60-臺中區監理所\",\"61-臺中市監理站\",\"62-南投監理站埔里分站\",\"63-豐原監理站\",\"64-彰化監理站\",\"65-南投監理站\"];\n",
    "[\"70-嘉義區監理所\",\"71-東勢監理分站\",\"72-雲林監理站\",\"73-新營監理站\",\"74-臺南監理站\",\"75-麻豆監理站\",\"76-嘉義市監理站\"];\n",
    "[\"30-高雄市區監理所\",\"31-苓雅監理站\",\"33-旗山監理站\"];\n",
    "[\"80-高雄區監理所\",\"81-臺東監理站\",\"82-屏東監理站\",\"83-恆春監理分站\",\"84-澎湖監理站\"]\"\"\"\n",
    "\n",
    "# Initialize a dictionary to store station information\n",
    "station_dict = {}\n",
    "\n",
    "# Regular expression pattern to match items inside []\n",
    "pattern = r'\\[([^]]+)\\]'\n",
    "\n",
    "# Loop through the split text and extract lists inside []\n",
    "for idx, item in enumerate(station_text.split(';')):\n",
    "    match = re.search(pattern, item)\n",
    "    if match:\n",
    "        extracted = eval(match.group(1))\n",
    "        for i in extracted:\n",
    "            station_code, station_name = i.split('-')\n",
    "            section_code = list(section_list[idx].keys())[0]\n",
    "            station_dict[station_name] = {\"stationCode\": station_code, \"sectionCode\": section_code}\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_number_of_pages(url, headers, **params):\n",
    "    \"\"\"\n",
    "    Extract the number of pages from the specified URL for a given set of parameters.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL pattern for querying auction data.\n",
    "        headers (dict): HTTP headers for the request.\n",
    "        **params: Additional parameters for the URL.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of pages with auction data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url.format(page=1, **params), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = soup.find(\"span\", id=\"pagebanner\").get_text()\n",
    "    numbers = [char for char in text.split('，')[0] if char.isdigit()]\n",
    "    number = int(''.join(numbers))\n",
    "\n",
    "    return number\n",
    "\n",
    "def scrape_page_data(url, headers, page, action, **params):\n",
    "    \"\"\"\n",
    "    Scrape data from a specific page of the Taiwan Motor Vehicle Driver Information Service (MVDis) website.\n",
    "    \n",
    "    This function sends a GET request to the provided URL with the specified page number and parameters.\n",
    "    It then extracts auction or bidding data from the page's HTML content based on the action.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL pattern for querying the MVDis website.\n",
    "        headers (dict): The headers for the HTTP request.\n",
    "        page (int): The page number to scrape.\n",
    "        action (str): The action to perform: 'bidding' or 'auction_records'.\n",
    "        **params: Additional parameters to include in the URL for customization.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing scraped data for each row on the page.\n",
    "    \"\"\"\n",
    "    print(page)\n",
    "    response = requests.get(url.format(page=page, **params), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    rows = soup.find_all('tr', class_=lambda x: x and (\"even\" in x or \"odd\" in x))\n",
    "    page_data = []\n",
    "    if action == 'bidding':\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            car_number = cols[0].text\n",
    "            number = car_number[-4:]\n",
    "            price = cols[3].text\n",
    "            page_data.append({\"car_number\": car_number, \"number\": number, \"price\": price, \"page\": page})\n",
    "    else:\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            car_number = cols[0].text\n",
    "            number = car_number[-4:]\n",
    "            price = cols[4].text\n",
    "            page_data.append({\"car_number\": car_number, \"number\": number, \"price\": price, \"page\": page})\n",
    "\n",
    "    return page_data\n",
    "    \n",
    "# 競標\n",
    "def get_bidding_by_station(**params):\n",
    "    \"\"\"\n",
    "    Scrape and analyze bidding data for a specific Taiwan Motor Vehicle Driver Information Service (MVDis) station.\n",
    "    \n",
    "    This function retrieves bidding data from the MVDis website for the specified station using the provided parameters.\n",
    "    It then analyzes the car numbers and prices, calculates the sum of the last four digits of each car number,\n",
    "    and determines if each car number is a lucky number based on a predefined lucky number DataFrame.\n",
    "    The results are saved to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        **params: Parameters including 'stationCode' for the station number.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the scraped and analyzed bidding data.\n",
    "    \"\"\"\n",
    "    action = 'bidding'\n",
    "    headers = {\n",
    "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36\"\n",
    "    }\n",
    "    url = \"https://www.mvdis.gov.tw/m3-emv-plate/bid/queryBiding?d-5481-p={page}&stationCode={stationCode}&onChangeItem=2&method=doChangeStation&sectionCode=4#gsc.tab=0\"\n",
    "    data_types = {'Column1': int, '類別': str, '號碼': str, '總和': str}\n",
    "    lucky_number = pd.read_excel(\"./result/lucky_number.xlsx\", dtype=data_types)\n",
    "    \n",
    "    output_data = []\n",
    "    total_pages = extract_number_of_pages(url, headers, **params)\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        output_data.extend(scrape_page_data(url, headers, page, action, **params))\n",
    "    \n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    output_df = output_df.merge(lucky_number, how='left', left_on='number', right_on='號碼')\n",
    "    output_df = output_df.sort_values(by='car_number')\n",
    "    output_df.to_excel(f\"./result/Bidding_{params['stationCode']}.xlsx\", index=False)\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# 競標紀錄\n",
    "def get_auction_records_by_station(**params):\n",
    "    \"\"\"\n",
    "    Scrape and analyze auction record data for a specific Taiwan Motor Vehicle Driver Information Service (MVDis) station.\n",
    "    \n",
    "    This function retrieves auction record data from the MVDis website for the specified station using the provided parameters.\n",
    "    It then analyzes the car numbers and prices, calculates the sum of the last four digits of each car number,\n",
    "    and determines if each car number is a lucky number based on a predefined lucky number DataFrame.\n",
    "    The results are saved to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "        **params: Parameters including 'stationCode', 'queryYear', 'queryMonth', 'sectionCode', and 'queryDate'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the scraped and analyzed auction record data.\n",
    "    \"\"\"\n",
    "    action = 'auction_records'\n",
    "    headers = {\n",
    "        'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36\"\n",
    "    }\n",
    "    url = \"https://www.mvdis.gov.tw/m3-emv-plate/bid/queryBid?queryType=2&CSRFToken=6fe10d2a-a0fb-479b-b650-2637d59e6a71&d-5481-p={page}&stationCode={stationCode}&onChangeItem=3&queryYear={queryYear}&queryMonth={queryMonth}&method=doChangeStation&sectionCode={sectionCode}&queryDate={queryDate}&queryBidType=2#gsc.tab=0\"\n",
    "    data_types = {'Column1': int, '類別': str, '號碼': str, '總和': str}\n",
    "    lucky_number = pd.read_excel(\"./result/lucky_number.xlsx\", dtype=data_types)\n",
    "\n",
    "    output_data = []\n",
    "    total_pages = extract_number_of_pages(url, headers, **params)\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        output_data.extend(scrape_page_data(url, headers, page, action, **params))\n",
    "    \n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    output_df = output_df.merge(lucky_number, how='left', left_on='number', right_on='號碼')\n",
    "    output_df = output_df.sort_values(by='car_number')\n",
    "    output_df.to_excel(f\"./result/Auction_Records_{params['stationCode']}.xlsx\", index=False)\n",
    "    \n",
    "    return output_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff127e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidding Example usage\n",
    "station_name = '宜蘭監理站'\n",
    "station_info = station_dict.get(station_name, {})\n",
    "station_code = station_info.get('stationCode')\n",
    "params = {'stationCode': station_code}\n",
    "result_df = get_bidding_by_station(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd9b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "target_number = '1389'\n",
    "filtered_df = result_df[result_df['number'] == target_number]\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auction Records Example usage\n",
    "station_name = '南投監理站'\n",
    "station_info = station_dict.get(station_name, {})\n",
    "queryYear=112\n",
    "queryMonth=8\n",
    "queryDate=16\n",
    "params = {\n",
    "    'stationCode': station_info.get('stationCode'), \n",
    "    'sectionCode': station_info.get('sectionCode'),\n",
    "    'queryYear':queryYear,\n",
    "    'queryMonth':queryMonth,\n",
    "    'queryDate':queryDate,\n",
    "    }\n",
    "result_df = get_auction_records_by_station(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "target_number = '1358'\n",
    "filtered_df = result_df[result_df['number'] == target_number]\n",
    "print(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
